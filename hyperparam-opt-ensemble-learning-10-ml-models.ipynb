{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/handeatasagun/hyperparam-opt-ensemble-learning-10-ml-models?scriptVersionId=144760752\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# From Vine to Palate: üç∑\n# Wine Quality Prediction through Machine Learning Models","metadata":{}},{"cell_type":"markdown","source":"## Business Problem\n\nThe key challenge in the wine industry is predicting people's wine taste preferences in advance. This is crucial for making informed decisions in wine production, marketing, and creating targeted consumer strategies. This problem can be addressed by analyzing a substantial dataset linked to physicochemical and sensory tests during the certification phase. Such analyses can help wine producers understand taste preferences, improve production processes, and inform niche market marketing strategies. This business issue can be resolved through machine learning techniques.\n\n## Dataset Story\n\nThe dataset pertains to various types of Portuguese 'Vinho Verde' red wines. For more information, you can refer to the study by Cortez et al.,titled 'Modeling wine preferences by data mining from physicochemical properties' (2009). Due to privacy and logistical constraints, only physicochemical variables are available (there is no data on factors such as grape types, wine brands, wine sale prices, etc.)\n\nThe dataset consists of the following features:\n- fixed acidity\n- volatile acidity\n- citric acid\n- residual sugar\n- chlorides\n- free sulfur dioxide\n- total sulfur dioxide\n- density\n- pH\n- sulphates\n- alcohol\n\nThe output variable, which we aim to predict, is the wine's __quality__, scored between 0 and 10.","metadata":{}},{"cell_type":"markdown","source":"# 1. Importing Required Libraries and Datasets","metadata":{}},{"cell_type":"markdown","source":"## 1.1. Importing libraries:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import StandardScaler\nfrom IPython.display import display, HTML\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import cross_validate, cross_val_predict\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import VotingClassifier\nfrom itertools import combinations\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\npd.set_option('display.width', 500)\n\nimport warnings\nwarnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:12.003935Z","iopub.execute_input":"2023-09-30T08:44:12.004275Z","iopub.status.idle":"2023-09-30T08:44:13.350674Z","shell.execute_reply.started":"2023-09-30T08:44:12.004249Z","shell.execute_reply":"2023-09-30T08:44:13.349218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Reading dataset:","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:13.352133Z","iopub.execute_input":"2023-09-30T08:44:13.352396Z","iopub.status.idle":"2023-09-30T08:44:13.36171Z","shell.execute_reply.started":"2023-09-30T08:44:13.352377Z","shell.execute_reply":"2023-09-30T08:44:13.360607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2. Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"markdown","source":"## 2.1. Data frame inspection function:\n\nThis function is designed to inspect and provide a summary of the essential characteristics and information of a dataframe.","metadata":{}},{"cell_type":"code","source":"def check_df(dataframe, head=5):\n    display(HTML(f\"<h3>Types</h3>{dataframe.dtypes.to_frame().to_html()}\"))\n    display(HTML(f\"<h3>Head</h3>{dataframe.head(head).to_html()}\"))\n    display(HTML(f\"<h3>Shape</h3>{dataframe.shape}\"))\n    display(HTML(f\"<h3>NA</h3>{dataframe.isnull().sum().to_frame().to_html()}\"))\n    display(HTML(f\"<h3>Quantiles</h3>{dataframe.describe([0.25, 0.50, 0.95]).T.to_html()}\"))\n\n    \ncheck_df(df)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:13.362397Z","iopub.execute_input":"2023-09-30T08:44:13.362599Z","iopub.status.idle":"2023-09-30T08:44:13.409638Z","shell.execute_reply.started":"2023-09-30T08:44:13.362582Z","shell.execute_reply":"2023-09-30T08:44:13.407879Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The dataset consists of 1599 rows and 12 columns. \n- All variables are numerical. \n- There are no missing values in any of the columns. \n- It is suspected that there may be outliers in some of the variables.","metadata":{}},{"cell_type":"markdown","source":"## 2.2. Detecting dataframe column types function:\nThis function analyzes the columns in a dataframe and determines categorical, numerical, and other columns.\n\nParameters:\n- cat_th (int): Threshold value for considering a column as categorical.\n- car_th (int): Threshold value for considering a column as having high cardinality.","metadata":{}},{"cell_type":"code","source":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() >= car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    \n    return cat_cols, num_cols, cat_but_car\n\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:13.413778Z","iopub.execute_input":"2023-09-30T08:44:13.414092Z","iopub.status.idle":"2023-09-30T08:44:13.42664Z","shell.execute_reply.started":"2023-09-30T08:44:13.414066Z","shell.execute_reply":"2023-09-30T08:44:13.425171Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Out of the 12 variables in the dataset, 11 are numerical variables, while 1 variable is numerical but actually categorical (\"quality').","metadata":{}},{"cell_type":"markdown","source":"## 2.3. Categorical column summary function:\n\nThe function visualizes the frequency and percentage of a categorical column.\n","metadata":{}},{"cell_type":"code","source":"def cat_summary(dataframe, col_name, plot=False):\n    if plot:\n        sns.set_palette(\"Set2\")\n        plt.figure(figsize=(8, 4))\n        ax = sns.countplot(y=col_name, data=dataframe, order=dataframe[col_name].value_counts().index)\n        \n        for p in ax.patches:\n            width = p.get_width()\n            ax.annotate(f'{width / len(dataframe) * 100:.1f}%', (width, p.get_y() + p.get_height() / 2.),\n                        ha='left', va='center')\n        \n        plt.show(block=True)\n        \n\nfor col in cat_cols:\n    cat_summary(df, col, plot=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:13.42822Z","iopub.execute_input":"2023-09-30T08:44:13.428667Z","iopub.status.idle":"2023-09-30T08:44:13.603871Z","shell.execute_reply.started":"2023-09-30T08:44:13.428642Z","shell.execute_reply":"2023-09-30T08:44:13.603023Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The values of the \"quality\" variable are mostly 5 or 6. This indicates that the examined wines are of **medium** quality.","metadata":{}},{"cell_type":"markdown","source":"## 2.4. Numerical column summary function:\n\nThe function visualizes the frequency and percentage of a numerical column.\n","metadata":{}},{"cell_type":"code","source":"def num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    summary = dataframe[numerical_col].describe(quantiles)\n\n    if plot:\n        fig, axes = plt.subplots(1, 2, figsize=(18, 4))\n        \n        # Plot histogram\n        dataframe[numerical_col].hist(bins=20, ax=axes[0], color=\"lightblue\")\n        axes[0].set_xlabel(numerical_col)\n        axes[0].set_title(numerical_col)\n        \n        # Display summary statistics as text\n        summary_text = \"\\n\".join([f'{col}: {value:.3f}' for col, value in summary.items()])\n        axes[1].text(0.5, 0.5, summary_text, fontsize=12, va='center', ha='left', linespacing=1.5)\n        axes[1].axis('off')  # Hide axis for the summary table\n        \n        plt.show()\n        \n\nfor col in num_cols:\n    num_summary(df, col, plot = True)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:13.604683Z","iopub.execute_input":"2023-09-30T08:44:13.604865Z","iopub.status.idle":"2023-09-30T08:44:16.171128Z","shell.execute_reply.started":"2023-09-30T08:44:13.604849Z","shell.execute_reply":"2023-09-30T08:44:16.169784Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Except for \"__density__\" and \"__pH__,\" it is observed that all other variables in the dataset are generally __right-skewed__.","metadata":{}},{"cell_type":"markdown","source":"## 2.5. Outlier detection:\n\nIn this section, we will try to identify outliers in each dataframe and take necessary actions if they exist. ","metadata":{}},{"cell_type":"code","source":"# The function is used to calculate the outlier thresholds for a specific column in a dataframe.\n################################################################\ndef outlier_thresholds(dataframe, col_name, q1=0.05, q3=0.95):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\n\n# This function is designed to check for outliers.\n################################################################\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False\n\n    \n# The function is designed to identify and retrieve outlier data points in a specified column of a dataframe \n# based on predefined outlier thresholds.\n################################################################\ndef grab_outliers(dataframe, col_name, index=False):\n    low, up = outlier_thresholds(dataframe, col_name)\n\n    outlier_df = dataframe[((dataframe[col_name] < low) | (dataframe[col_name] > up))]\n\n    if outlier_df.shape[0] > 10:\n        display(outlier_df.head())\n    else:\n        display(outlier_df)\n\n    if index:\n        outlier_index = outlier_df.index\n        return outlier_index\n\n    return outlier_df.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.172581Z","iopub.execute_input":"2023-09-30T08:44:16.172836Z","iopub.status.idle":"2023-09-30T08:44:16.179627Z","shell.execute_reply.started":"2023-09-30T08:44:16.172813Z","shell.execute_reply":"2023-09-30T08:44:16.178482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in num_cols:\n    print(col, check_outlier(df, col))","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.18068Z","iopub.execute_input":"2023-09-30T08:44:16.180912Z","iopub.status.idle":"2023-09-30T08:44:16.224089Z","shell.execute_reply.started":"2023-09-30T08:44:16.180893Z","shell.execute_reply":"2023-09-30T08:44:16.223269Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- When examining the dataset for outliers, it was observed that the variables \"__residual sugar__\", \"__chlorides__\", \"__total sulfur dioxide__\", and \"__sulphates__\" contain outliers.","metadata":{}},{"cell_type":"code","source":"for col in num_cols:\n    print(f\"Outliers in {col}:\")\n    grab_outliers(df, col)\n    print(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.224916Z","iopub.execute_input":"2023-09-30T08:44:16.225126Z","iopub.status.idle":"2023-09-30T08:44:16.31627Z","shell.execute_reply.started":"2023-09-30T08:44:16.225108Z","shell.execute_reply":"2023-09-30T08:44:16.314976Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There are a few outliers in the \"__total sulfur dioxide__\" and \"__sulphates__\" variables, while there are more than 10 outliers in the \"__residual sugar__\" and \"__chlorides__\" variables.\n- Outliers will be processed using the __Local Outlier Factor (LOF) method__. It is an outlier detection algorithm that identifies unusual or isolated data points based on their local densities, with points having high LOF values typically considered as outliers.","metadata":{}},{"cell_type":"code","source":"# Detecting Outliers with Local Outlier Factor (LOF) method\n###########################################################\n\n# Initialize the LOF model and consider 20 neighbors for each data point\nclf = LocalOutlierFactor(n_neighbors=20)\nclf.fit_predict(df)\n\n# Get the LOF scores\ndf_scores = clf.negative_outlier_factor_\n\n# Transfer LOF scores into a sorted dataframe\nscores = pd.DataFrame(np.sort(df_scores))\n\n# Create a plot showing LOF scores within the first 50 data points\nscores.plot(stacked=True, xlim=[0, 50], style='.-')\nplt.show()\n\n# Set a threshold for detecting outliers\nth = np.sort(df_scores)[4]\n\n# Remove outliers\nrows_to_drop = df[df_scores < th].index\ndf.drop(axis=0, index=rows_to_drop, inplace=True)\ndf.shape","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.318798Z","iopub.execute_input":"2023-09-30T08:44:16.319104Z","iopub.status.idle":"2023-09-30T08:44:16.498974Z","shell.execute_reply.started":"2023-09-30T08:44:16.319081Z","shell.execute_reply":"2023-09-30T08:44:16.497365Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Based on the elbow method, it was determined that there are __4 outliers__, and these rows have been __removed__ from the dataset.","metadata":{}},{"cell_type":"markdown","source":"## 2.6. Detecting missing values function:\n\nThe function generates a table summarizing missing values in the given dataframe.","metadata":{}},{"cell_type":"code","source":"def missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n\n    if na_name:\n        return na_columns\n\nmissing_values_table(df)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.499825Z","iopub.execute_input":"2023-09-30T08:44:16.500056Z","iopub.status.idle":"2023-09-30T08:44:16.512948Z","shell.execute_reply.started":"2023-09-30T08:44:16.500036Z","shell.execute_reply":"2023-09-30T08:44:16.51172Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As previously mentioned, there are no missing values in the dataset.","metadata":{}},{"cell_type":"markdown","source":"## 2.7. Correlation heatmap function:\n\nThis function plots a correlation heatmap for the given dataframe.","metadata":{}},{"cell_type":"code","source":"def plot_correlation_heatmap(dataframe, threshold=0.5, title=\"Correlation Heatmap\"):\n    corr = dataframe.corr()\n    high_corr = corr[abs(corr) >= threshold]\n    mask = np.triu(np.ones_like(high_corr, dtype=bool))\n    fig, ax = plt.subplots(figsize=(8, 6))\n    vmin = -1.0\n    vmax = 1.0\n    sns.heatmap(high_corr, annot=True, fmt=\".2f\", cmap='coolwarm', mask=mask, \n                linewidths=0.5, ax=ax, vmin=vmin, vmax=vmax, cbar_kws={'label': 'Correlation'})\n    ax.set_title(title)\n    plt.show()\n    \n    \nplot_correlation_heatmap(df)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.514673Z","iopub.execute_input":"2023-09-30T08:44:16.515623Z","iopub.status.idle":"2023-09-30T08:44:16.825459Z","shell.execute_reply.started":"2023-09-30T08:44:16.515575Z","shell.execute_reply":"2023-09-30T08:44:16.824623Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- An increase in citric acid is associated with an increase in fixed acidity and a decrease in volatile acidity.\n- As total sulfur dioxide increases, free sulfur dioxide also increases.\n- Fixed acidity has a positive correlation with density and a negative correlation with pH. Similarly, citric acid has a negative correlation with pH.\n- Since the correlation coefficients are not very high, there are no strong relationships between variables. Therefore, no variable has been removed from the dataset.","metadata":{}},{"cell_type":"markdown","source":"# 3. Feature Engineering","metadata":{}},{"cell_type":"markdown","source":"## 3.1. Creating new features:","metadata":{}},{"cell_type":"code","source":"df['_alcohol_x_sulphates_'] = df['alcohol'] * df['sulphates']\n\ndf['_alcohol_density_ratio_'] = df['alcohol'] / df['density']\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.826443Z","iopub.execute_input":"2023-09-30T08:44:16.826908Z","iopub.status.idle":"2023-09-30T08:44:16.83973Z","shell.execute_reply.started":"2023-09-30T08:44:16.826881Z","shell.execute_reply":"2023-09-30T08:44:16.838182Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Before creating these variables, models were run without adding any variables, and the top 5 features were determined according to feature importance. Using these features, these two variables were created.","metadata":{}},{"cell_type":"markdown","source":"## 3.2. Encoding processes:","metadata":{}},{"cell_type":"markdown","source":"This process is done to convert categorical variables into numerical ones. In fact, the \"__quality__\" categorical variable in this dataset is already expressed numerically. However, this section has been added to perform the necessary operations on any __rare groups__ in this variable.","metadata":{}},{"cell_type":"code","source":"def rare_analyser(dataframe, target, cat_cols):\n    for col in cat_cols:\n        print(col, \":\", len(dataframe[col].value_counts()))\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO\": dataframe[col].value_counts() / len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(col)[target].mean()}), end=\"\\n\\n\\n\")\n\nrare_analyser(df, \"quality\", cat_cols)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.841085Z","iopub.execute_input":"2023-09-30T08:44:16.841353Z","iopub.status.idle":"2023-09-30T08:44:16.860278Z","shell.execute_reply.started":"2023-09-30T08:44:16.841328Z","shell.execute_reply":"2023-09-30T08:44:16.858793Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- As a result of the rare analysis, it was observed that some groups had very low percentages. \n- Based on this, a decision was made to classify the groups that received __3, 4, and 5__ points as '__0__\", and those that received __6, 7, and 8__ points as '__1__', thereby categorizing wine quality only as 'good' and 'bad'.","metadata":{}},{"cell_type":"code","source":"df[\"quality\"] = np.where(df[\"quality\"] > 5, 1, 0)\ndf[\"quality\"].head()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.862269Z","iopub.execute_input":"2023-09-30T08:44:16.862788Z","iopub.status.idle":"2023-09-30T08:44:16.877633Z","shell.execute_reply.started":"2023-09-30T08:44:16.862756Z","shell.execute_reply":"2023-09-30T08:44:16.87662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3.3. Standardization:","metadata":{}},{"cell_type":"markdown","source":"The process is to make variables with different scales or distributions have the same scale or distribution.","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\n\nnum_cols = [col for col in df.columns if col != 'quality']\n\ndf[num_cols] = scaler.fit_transform(df[num_cols])\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.87925Z","iopub.execute_input":"2023-09-30T08:44:16.879605Z","iopub.status.idle":"2023-09-30T08:44:16.902645Z","shell.execute_reply.started":"2023-09-30T08:44:16.879578Z","shell.execute_reply":"2023-09-30T08:44:16.901882Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 4. Modelling","metadata":{}},{"cell_type":"markdown","source":"In this section, __10__ different ML models will be evaluated suitable for a classification problem. __Hyperparameter optimization__ will be applied to these models, and __ensemble learning__ will be performed for the models that yield the best results.","metadata":{}},{"cell_type":"code","source":"X = df.drop(\"quality\", axis=1)\ny = df['quality']","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.903771Z","iopub.execute_input":"2023-09-30T08:44:16.904099Z","iopub.status.idle":"2023-09-30T08:44:16.910059Z","shell.execute_reply.started":"2023-09-30T08:44:16.904073Z","shell.execute_reply":"2023-09-30T08:44:16.908798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"check_df(X)  # The independent variables have been checked.","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.911109Z","iopub.execute_input":"2023-09-30T08:44:16.911426Z","iopub.status.idle":"2023-09-30T08:44:16.95888Z","shell.execute_reply.started":"2023-09-30T08:44:16.911407Z","shell.execute_reply":"2023-09-30T08:44:16.957904Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4.1. Base models evaluation function:","metadata":{}},{"cell_type":"markdown","source":"The __evaluate_model function__ assesses the performance of a given machine learning model using cross-validation on a dataset, calculating key classification metrics such as __accuracy__, __F1 score__, and __ROC AUC__. The results are aggregated into a dataframe and sorted to identify the best-performing models.","metadata":{}},{"cell_type":"code","source":"def evaluate_model(model, X, y):\n    cv_results = cross_validate(model, X, y, cv=5, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n    report = classification_report(y, cross_val_predict(model, X, y, cv=5))\n    mean_accuracy = cv_results['test_accuracy'].mean()\n    mean_f1 = cv_results['test_f1'].mean()\n    mean_roc_auc = cv_results['test_roc_auc'].mean()\n    return {\n        \"report\": report,\n        \"accuracy_mean\": mean_accuracy,\n        \"f1_mean\": mean_f1,\n        \"roc_auc_mean\": mean_roc_auc\n    }\n\nmodels = [('KNN', KNeighborsClassifier()),\n    ('LR', LogisticRegression(random_state=17)),\n    (\"CART\", DecisionTreeClassifier(random_state=17)),\n    (\"SVC\", SVC(random_state=17)),\n    (\"RF\", RandomForestClassifier(random_state=17)),\n    ('Adaboost', AdaBoostClassifier(random_state=17)),\n    ('GBM', GradientBoostingClassifier(random_state=17)),\n    ('LGBM', LGBMClassifier(random_state=17)),\n    ('CatBoost', CatBoostClassifier(random_state=17, verbose=False)),\n    ('XGBoost', XGBClassifier(random_state=17))\n]\n\n\nresults_df = pd.DataFrame(columns=[\"Model\", \"Accuracy\", \"F1 Score\", \"ROC AUC\"])\n\nresults = []\n\nfor model_name, model in models:\n    model_results = evaluate_model(model, X, y)\n    results.append({\n        \"Model\": model_name,\n        \"Accuracy\": model_results[\"accuracy_mean\"],\n        \"F1 Score\": model_results[\"f1_mean\"],\n        \"ROC AUC\": model_results[\"roc_auc_mean\"]\n    })\n\nresults_df = pd.DataFrame(results)\nresults_df = results_df.sort_values(by=\"Accuracy\", ascending=False)\nprint(results_df)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:16.959917Z","iopub.execute_input":"2023-09-30T08:44:16.960102Z","iopub.status.idle":"2023-09-30T08:44:49.696168Z","shell.execute_reply.started":"2023-09-30T08:44:16.960086Z","shell.execute_reply":"2023-09-30T08:44:49.695199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- When we rank the models based on accuracy, we observe that the top 3 models are **CatBoost**, **LR** and **SVC**.\n- The models whose accuracy falls **below 0.7** are **KNN**, and **CART**.","metadata":{}},{"cell_type":"markdown","source":"## 4.2. Feature importance function:","metadata":{}},{"cell_type":"markdown","source":"This function calculates and plots the importance scores of the features, helping to identify which features have the most significant impact on the model's predictions. ","metadata":{}},{"cell_type":"code","source":"def plot_importance(model, features, num=len(X)):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_,\n                                'Feature': features.columns})\n    \n    plt.figure(figsize=(8, 4))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\",\n                data=feature_imp.sort_values(by=\"Value\",\n                                             ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n\n\n# CatBoost -> high accuracy\ncatboost_model = models[8][1]\ncatboost_model.fit(X, y)\n\nplot_importance(catboost_model, X)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:49.697827Z","iopub.execute_input":"2023-09-30T08:44:49.698223Z","iopub.status.idle":"2023-09-30T08:44:51.601063Z","shell.execute_reply.started":"2023-09-30T08:44:49.698202Z","shell.execute_reply":"2023-09-30T08:44:51.599855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The fact that the new variables resulting from the interaction of variables with each other are among the top 3 most important variables suggests that these variables make a significant contribution to the model's performance.","metadata":{}},{"cell_type":"markdown","source":"## 4.3. Automated hyperparameter optimization:","metadata":{}},{"cell_type":"markdown","source":"Hyperparameter optimization was performed on machine learning models to achieve their best performance. This process was automated using the '**hyperparameter_optimization function**'.","metadata":{}},{"cell_type":"code","source":"# Hyperparameter grids:\n#######################\n\nknn_params = {\"n_neighbors\": range(2, 50)}\n\ncart_params = {'max_depth': range(1, 20),\n               \"min_samples_split\": range(2, 30)}\n\nrf_params = {\"max_depth\": [10, 12, 15],\n             \"max_features\": [0.5, 1, 2],\n             \"min_samples_split\": [12, 15, 17],\n             \"n_estimators\": [310, 320, 330]}\n\nxgboost_params = {\"learning_rate\": [0.035, 0.03, 0.025],\n                  \"max_depth\": [3, 4, 5],\n                  \"n_estimators\": [190, 200, 210]}\n\nlightgbm_params = {\"learning_rate\": [0.004, 0.005, 0.006],\n                   \"n_estimators\": [280, 300, 320]}\n\nsvc_params = {\"C\": [1.1, 1.2, 1.3],\n              \"kernel\": [\"linear\", \"rbf\"]}\n\nlr_params = {\"C\": [0.13, 0.14, 0.15],\n             \"solver\": [\"liblinear\", \"lbfgs\"]}\n\nadaboost_params = {\"n_estimators\": [70, 75, 80],\n                   \"learning_rate\": [0.1, 0.11, 0.12]}\n\ngbm_params = {\"n_estimators\": [14, 15, 16],\n              \"learning_rate\": [0.04, 0.05, 0.06],\n              \"max_depth\": [1, 2, 3]}\n\ncatboost_params = {\"iterations\": [450, 460, 470],\n                   \"learning_rate\": [0.005, 0.006, 0.007],\n                   \"depth\": [2, 3, 4]}","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:51.602661Z","iopub.execute_input":"2023-09-30T08:44:51.602966Z","iopub.status.idle":"2023-09-30T08:44:51.611163Z","shell.execute_reply.started":"2023-09-30T08:44:51.602941Z","shell.execute_reply":"2023-09-30T08:44:51.609947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification algorithms:\n############################\n\nclassifiers = [('KNN', KNeighborsClassifier(), knn_params),\n    (\"CART\", DecisionTreeClassifier(random_state=17), cart_params),\n    (\"RF\", RandomForestClassifier(random_state=17), rf_params),\n    ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=17), xgboost_params),\n    ('LightGBM', LGBMClassifier(random_state=17), lightgbm_params),\n    ('SVC', SVC(random_state=17, probability=True), svc_params),\n    ('LR', LogisticRegression(random_state=17), lr_params),\n    ('Adaboost', AdaBoostClassifier(random_state=17), adaboost_params),\n    ('GBM', GradientBoostingClassifier(random_state=17), gbm_params),\n    ('CatBoost', CatBoostClassifier(verbose=False, random_state=17), catboost_params),\n]","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:51.612479Z","iopub.execute_input":"2023-09-30T08:44:51.612995Z","iopub.status.idle":"2023-09-30T08:44:51.629578Z","shell.execute_reply.started":"2023-09-30T08:44:51.612962Z","shell.execute_reply":"2023-09-30T08:44:51.628255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def hyperparameter_optimization(X, y, cv=5, scoring=\"accuracy\"):\n    print(\"Hyperparameter Optimization....\")\n    best_models = {}\n    for name, classifier, params in classifiers:\n        print(f\"########## {name} ##########\")\n        cv_results = cross_validate(classifier, X, y, cv=cv, scoring=scoring)\n        print(f\"{scoring} (Before): {round(cv_results['test_score'].mean(), 4)}\")\n\n        gs_best = GridSearchCV(classifier, params, cv=cv, n_jobs=-1, verbose=False).fit(X, y)\n        final_model = classifier.set_params(**gs_best.best_params_)\n\n        cv_results = cross_validate(final_model, X, y, cv=cv, scoring=scoring)\n        print(f\"{scoring} (After): {round(cv_results['test_score'].mean(), 4)}\")\n        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n        best_models[name] = final_model\n    return best_models\n\nbest_models = hyperparameter_optimization(X, y)","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:44:51.632921Z","iopub.execute_input":"2023-09-30T08:44:51.633192Z","iopub.status.idle":"2023-09-30T08:54:29.402061Z","shell.execute_reply.started":"2023-09-30T08:44:51.633172Z","shell.execute_reply":"2023-09-30T08:54:29.400904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The optimization process has resulted in expected improvements in the performance of all models, with all models achieving an accuracy value exceeding 0.7 except for CART. It has been observed that the model with the highest accuracy is __CatBoost (0.7561)__.","metadata":{}},{"cell_type":"markdown","source":"## 4.4. Ensemble learning:","metadata":{}},{"cell_type":"markdown","source":"We apply ensemble learning to combine multiple machine learning models to improve the overall performance of the model and achieve better predictions. All models were tested in triple combinations, and the combination that yielded the highest accuracy value was determined.","metadata":{}},{"cell_type":"code","source":"best_accuracy = 0\nbest_combination = None\nbest_classifiers = None\n\ncombination_sizes = [3]  # [2, 3, 4, ...] combinations of 2 or more models can be tried\n\nfor size in combination_sizes:\n    for combination in combinations([name for name, _, _ in classifiers], size):\n        combination_name = \"_\".join(combination)\n        selected_classifiers = [(name, model, params) for name, model, params in classifiers if name in combination]\n\n        combined_estimators = [(name, model) for name, model, _ in selected_classifiers]\n        combined_classifier = VotingClassifier(estimators=combined_estimators, voting='soft')\n\n        cv_results = cross_validate(combined_classifier, X, y, cv=5, scoring=\"accuracy\")\n        current_accuracy = cv_results['test_score'].mean()\n\n        if current_accuracy > best_accuracy:\n            best_accuracy = current_accuracy\n            best_combination = combination_name\n            best_classifiers = selected_classifiers\n\nprint(f\"Best Combination: {best_combination}\")\nprint(f\"Best Accuracy: {round(best_accuracy, 4)}\")\n\n\n# VotingClassifier\nbest_estimators = [(name, model) for name, model, _ in best_classifiers]\nbest_voting_clf = VotingClassifier(estimators=best_estimators, voting='soft')\nbest_voting_clf.fit(X, y)\n","metadata":{"execution":{"iopub.status.busy":"2023-09-30T08:54:29.403652Z","iopub.execute_input":"2023-09-30T08:54:29.403988Z","iopub.status.idle":"2023-09-30T09:05:13.402458Z","shell.execute_reply.started":"2023-09-30T08:54:29.403957Z","shell.execute_reply":"2023-09-30T09:05:13.401392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It has been observed that the combination of the models __Adaboost__, __GBM__, and __CatBoost__ yielded the best result among the used models, achieving a higher accuracy value (__0.7567__) compared to using the models individually.","metadata":{}},{"cell_type":"markdown","source":"# 5. Conclusion","metadata":{}},{"cell_type":"markdown","source":"In this study, it has been observed that the combination of \"__Adaboost, CatBoost, and GBM__\" determined through ensemble learning yielded the highest accuracy value. It has been decided to use this combination for predicting red wine quality.","metadata":{}},{"cell_type":"markdown","source":"## If you find this notebook useful then please upvote. Thank you in advance.","metadata":{}}]}